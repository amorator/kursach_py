===== D:\_Учеба\_курсач_питон\app\app.py =====
# -*- coding: utf-8 -*-
"""Streamlit-приложение для визуализации методов одномерной оптимизации."""
from __future__ import annotations

import streamlit as st
import numpy as np
import sympy as sp
from typing import Optional, Tuple

from optim.methods import OptimizationResult
from optim.selection import auto_select_and_run
from visualize import plot_history_1d


st.set_page_config(page_title="1D Optimization", layout="centered")
st.title("Визуализация методов одномерной оптимизации")

with st.sidebar:
	st.header("Параметры")
	f_text = st.text_input("Функция f(x)", value="(x-2)**2 + 3")
	var = sp.symbols("x")

	col1, col2 = st.columns(2)
	with col1:
		a = st.number_input("a (левая граница)", value=-5.0, step=0.5)
	with col2:
		b = st.number_input("b (правая граница)", value=5.0, step=0.5)
	if a >= b:
		st.warning("Требуется a < b")
	bounds: Optional[Tuple[float, float]] = (a, b)

	method = st.selectbox(
		"Метод",
		["Автовыбор", "Пассивный поиск", "Дихотомия", "Золотое сечение", "Касательных (Ньютона)", "Секущих"],
	)

	tol = st.number_input("Точность tol", value=1e-5, format="%e")
	samples = st.number_input("Число проб (для пассивного поиска)", value=50, min_value=2, step=1)

	st.divider()
	st.caption("Начальные приближения (для методов производных)")
	x0 = st.number_input("x0", value=0.0)
	x1 = st.number_input("x1 (для секущих)", value=1.0)

	st.divider()
	with st.expander("Производные (опционально)"):
		df_text = st.text_input("f'(x)", value="")
		d2f_text = st.text_input("f''(x)", value="")


def make_callable(expr_text: str):
	"""Преобразовать текст выражения в числовую функцию одной переменной.

	Возвращает кортеж: (f, sympy_expr), где
	- f: вызываемая функция f(x) для численных расчётов;
	- sympy_expr: символьное выражение для возможного дальнейшего анализа.
	"""
	if not expr_text.strip():
		raise ValueError("Пустое выражение функции")
	expr = sp.sympify(expr_text)
	var = sp.symbols("x")
	f = sp.lambdify(var, expr, modules=["numpy", "math"])
	return f, expr


run = st.button("Запустить оптимизацию")

if run:
	try:
		f, f_expr = make_callable(f_text)
		# Производные (если заданы)
		df = None
		d2f = None
		if df_text.strip():
			_, df_expr = make_callable(df_text)
			df = sp.lambdify(sp.symbols("x"), df_expr, modules=["numpy", "math"])
		if d2f_text.strip():
			_, d2f_expr = make_callable(d2f_text)
			d2f = sp.lambdify(sp.symbols("x"), d2f_expr, modules=["numpy", "math"])

		prefer = None
		if method == "Пассивный поиск":
			prefer = "passive"
		elif method == "Дихотомия":
			prefer = "dichotomy"
		elif method == "Золотое сечение":
			prefer = "golden"
		elif method == "Касательных (Ньютона)":
			prefer = "newton"
		elif method == "Секущих":
			prefer = "secant"

		res: OptimizationResult = auto_select_and_run(
			f=f,
			bounds=bounds,
			tol=float(tol),
			samples=int(samples) if method in ("Пассивный поиск", "Автовыбор") else None,
			df=df,
			d2f=d2f,
			x0=float(x0),
			x1=float(x1),
			prefer=prefer if method != "Автовыбор" else None,
		)

		st.success(f"Метод: {res.method}")
		st.write(f"x_min ≈ {res.x_min:.8g}")
		st.write(f"f(x_min) ≈ {res.f_min:.8g}")
		st.write(f"Итераций: {res.iterations}")

		fig = plot_history_1d(f, res.history, bounds=bounds, title=res.method)
		st.pyplot(fig)

		# Таблица итераций
		st.subheader("Таблица итераций")
		rows = []
		for i, step in enumerate(res.history, start=1):
			row = {"iter": i}
			# Приводим значения к числам там, где возможно
			for k, v in step.items():
				try:
					row[k] = float(v)
				except Exception:
					row[k] = v
			rows.append(row)
		st.dataframe(rows, use_container_width=True)
	except Exception as e:
		st.error(str(e))


===== D:\_Учеба\_курсач_питон\app\visualize.py =====
# -*- coding: utf-8 -*-
"""Визуализация истории итераций одномерной оптимизации."""
from __future__ import annotations

from typing import Callable, Dict, Any, List, Optional, Tuple
import numpy as np
import matplotlib.pyplot as plt


def plot_history_1d(
	f: Callable[[float], float],
	history: List[Dict[str, Any]],
	bounds: Optional[Tuple[float, float]] = None,
	title: str = "",
	points_only: bool = False,
):
	"""Построить график функции и отобразить историю итераций.

	Параметры:
		f: Callable[[float], float]
			Числовая функция одной переменной.
		history: List[Dict[str, Any]]
			Список шагов, сформированный методами оптимизации.
		bounds: Optional[Tuple[float, float]]
			Интервал визуализации функции. Если None — рисуются только точки.
		title: str
			Заголовок графика.
		points_only: bool
			Если True, функция не рисуется, только точки/интервалы.

	Возвращает:
		matplotlib.figure.Figure: объект фигуры с построенным графиком.
	"""
	fig, ax = plt.subplots(figsize=(7, 4))

	if bounds is not None and not points_only:
		a, b = bounds
		xs = np.linspace(a, b, 400)
		fx = [float(f(x)) for x in xs]
		ax.plot(xs, fx, label="f(x)", color="#1f77b4")

	# Рисуем историю
	for step in history:
		if "a" in step and "b" in step:
			a = float(step["a"])
			b = float(step["b"])
			ax.hlines(y=step.get("f", ax.get_ylim()[0]), xmin=a, xmax=b, colors="#ff7f0e", alpha=0.3)
			ax.vlines([a, b], *ax.get_ylim(), colors="#ff7f0e", alpha=0.05)
		if "x" in step:
			x = float(step["x"])
			ax.plot([x], [float(step.get("f", f(x)))], marker="o", color="#d62728", alpha=0.7)
		if "x1" in step and "x2" in step:
			x1 = float(step["x1"])
			x2 = float(step["x2"])
			ax.plot([x1, x2], [float(step.get("f1", f(x1))), float(step.get("f2", f(x2)))],
				marker="x", linestyle="none", color="#2ca02c", alpha=0.6)

	if title:
		ax.set_title(title)
	ax.set_xlabel("x")
	ax.set_ylabel("f(x)")
	ax.grid(True, alpha=0.2)
	ax.legend(loc="best")
	fig.tight_layout()
	return fig


===== D:\_Учеба\_курсач_питон\app\__init__.py =====
# -*- coding: utf-8 -*-
"""Пакет приложения Streamlit для визуализации методов одномерной оптимизации."""


===== D:\_Учеба\_курсач_питон\app\optim\methods.py =====
# -*- coding: utf-8 -*-
"""Методы одномерной оптимизации.

Содержит реализации и единый тип результата для визуализации итераций:
- passive_search: пассивный поиск (равномерное сканирование интервала)
- dichotomy: метод дихотомии
- golden_section: метод золотого сечения
- newton_tangent: метод касательных (Ньютона) по df и d2f
- secant_on_gradient: метод секущих, применяемый к уравнению f'(x)=0

Формат элементов history по методам:
- Пассивный поиск: {"x": float, "f": float}
- Дихотомия/золотое сечение: {"a": float, "b": float, "x1": float, "x2": float, "f1": float, "f2": float}
  Заключительная запись может быть {"a": float, "b": float, "x": float, "f": float}
- Ньютон: {"x": float, "df": float, "d2f": float, "f": float}
- Секущие: {"x": float, "g": float, "f": float}
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Callable, List, Tuple, Optional, Dict, Any
import numpy as np


@dataclass
class OptimizationResult:
	"""Результат работы метода оптимизации.

	Атрибуты:
		x_min: float
			Приближение к точке минимума.
		f_min: float
			Значение функции в точке минимума.
		iterations: int
			Количество выполненных итераций.
		history: List[Dict[str, Any]]
			История итераций для визуализации. Формат зависит от метода, но
			обычно содержит текущий интервал или точку и значение функции.
		method: str
			Название использованного метода.
	"""

	x_min: float
	f_min: float
	iterations: int
	history: List[Dict[str, Any]]
	method: str


def passive_search(
	f: Callable[[float], float],
	bounds: Tuple[float, float],
	samples: int = 50,
) -> OptimizationResult:
	"""Метод пассивного поиска (равномерное сканирование интервала).

	Метод равномерно разбивает отрезок [a, b] на заданное число точек и
	выбирает точку с наименьшим значением функции.

	Плюсы: простота, не требует дополнительных условий.
	Минусы: медленный, точность ограничена числом проб.

	Параметры:
		f: Callable[[float], float]
			Целевая функция одной переменной.
		bounds: Tuple[float, float]
			Интервал поиска (a, b), где a < b.
		samples: int, по умолчанию 50
			Количество равномерно распределенных пробных точек.

	Возвращает:
		OptimizationResult: результат оптимизации и история точек.

	Исключения:
		ValueError: если некорректен интервал или количество проб < 2.
	"""
	a, b = bounds
	if not (a < b):
		raise ValueError("Неверные границы интервала: требуется a < b")
	if samples < 2:
		raise ValueError("Число проб должно быть >= 2")

	xs = np.linspace(a, b, samples)
	history: List[Dict[str, Any]] = []
	f_vals = []
	for x in xs:
		fx = float(f(x))
		f_vals.append(fx)
		history.append({"x": float(x), "f": fx})

	idx = int(np.argmin(f_vals))
	x_min = float(xs[idx])
	f_min = float(f_vals[idx])
	return OptimizationResult(
		x_min=x_min,
		f_min=f_min,
		iterations=samples,
		history=history,
		method="Пассивный поиск",
	)


def dichotomy(
	f: Callable[[float], float],
	bounds: Tuple[float, float],
	tol: float = 1e-5,
	delta: Optional[float] = None,
	max_iter: int = 10_000,
) -> OptimizationResult:
	"""Метод дихотомии для поиска минимума на отрезке.

	На каждой итерации выбираются две близкие точки по разные стороны от
	середины интервала: x1 = m - δ, x2 = m + δ, где m — середина интервала.
	Сравнивая f(x1) и f(x2), сужаем интервал. Останавливаемся, когда длина
	интервала становится меньше tol или достигнут лимит итераций.

	Параметры:
		f: Callable[[float], float]
			Целевая функция.
		bounds: Tuple[float, float]
			Интервал (a, b), где a < b.
		tol: float, по умолчанию 1e-5
			Точность по длине интервала.
		delta: Optional[float]
			Малое положительное число для смещения от середины. Если None,
			берётся delta = tol / 2.
		max_iter: int
			Максимальное количество итераций.

	Возвращает:
		OptimizationResult: результат оптимизации с историей интервалов.

	Исключения:
		ValueError: при некорректных параметрах tol/delta или интервале.
	"""
	a, b = bounds
	if not (a < b):
		raise ValueError("Неверные границы интервала: требуется a < b")
	if tol <= 0:
		raise ValueError("tol должен быть > 0")
	if delta is None:
		delta = tol / 2.0
	if delta <= 0:
		raise ValueError("delta должен быть > 0")

	history: List[Dict[str, Any]] = []
	iterations = 0
	while (b - a) > tol and iterations < max_iter:
		m = (a + b) / 2.0
		x1 = m - delta
		x2 = m + delta
		f1 = float(f(x1))
		f2 = float(f(x2))
		history.append({"a": a, "b": b, "x1": x1, "x2": x2, "f1": f1, "f2": f2})
		if f1 < f2:
			b = x2
		else:
			a = x1
		iterations += 1

	x_min = (a + b) / 2.0
	f_min = float(f(x_min))
	history.append({"a": a, "b": b, "x": x_min, "f": f_min})
	return OptimizationResult(
		x_min=x_min,
		f_min=f_min,
		iterations=iterations,
		history=history,
		method="Дихотомия",
	)


def golden_section(
	f: Callable[[float], float],
	bounds: Tuple[float, float],
	tol: float = 1e-5,
	max_iter: int = 10_000,
) -> OptimizationResult:
	"""Метод золотого сечения для унимодальной функции на отрезке.

	Использует фиксированное отношение отрезков, основанное на золотом сечении,
	обеспечивая эффективное сужение интервала без пересчёта внутренних точек.

	Параметры:
		f: Callable[[float], float]
			Целевая функция.
		bounds: Tuple[float, float]
			Интервал (a, b), a < b.
		tol: float
			Критерий остановки по длине интервала.
		max_iter: int
			Максимальное количество итераций.

	Возвращает:
		OptimizationResult: результат с историей интервалов и точек.

	Исключения:
		ValueError: если интервал некорректен или tol <= 0.
	"""
	a, b = bounds
	if not (a < b):
		raise ValueError("Неверные границы интервала: требуется a < b")
	if tol <= 0:
		raise ValueError("tol должен быть > 0")

	phi = (1 + 5 ** 0.5) / 2
	resphi = 2 - phi

	x1 = a + resphi * (b - a)
	x2 = b - resphi * (b - a)
	f1 = float(f(x1))
	f2 = float(f(x2))

	history: List[Dict[str, Any]] = []
	iterations = 0
	while (b - a) > tol and iterations < max_iter:
		history.append({"a": a, "b": b, "x1": x1, "x2": x2, "f1": f1, "f2": f2})
		if f1 < f2:
			b, x2, f2 = x2, x1, f1
			x1 = a + resphi * (b - a)
			f1 = float(f(x1))
		else:
			a, x1, f1 = x1, x2, f2
			x2 = b - resphi * (b - a)
			f2 = float(f(x2))
		iterations += 1

	x_min = (a + b) / 2
	f_min = float(f(x_min))
	history.append({"a": a, "b": b, "x": x_min, "f": f_min})
	return OptimizationResult(
		x_min=x_min,
		f_min=f_min,
		iterations=iterations,
		history=history,
		method="Золотое сечение",
	)


def newton_tangent(
	f: Callable[[float], float],
	df: Callable[[float], float],
	d2f: Optional[Callable[[float], float]],
	x0: float,
	tol: float = 1e-6,
	max_iter: int = 100,
) -> OptimizationResult:
	"""Метод касательных (Ньютона) для поиска минимума.

	Итерационная схема: x_{k+1} = x_k - f'(x_k) / f''(x_k).
	Требует вычисления первой и второй производных. Если d2f не задан, метод
	не может быть применён напрямую.

	Параметры:
		f: Callable[[float], float]
			Целевая функция.
		df: Callable[[float], float]
			Первая производная функции.
		d2f: Optional[Callable[[float], float]]
			Вторая производная функции. Обязательна для метода Ньютона.
		x0: float
			Начальное приближение.
		tol: float
			Критерий остановки по норме шага и |f'(x)|.
		max_iter: int
			Максимум итераций.

	Возвращает:
		OptimizationResult: точка минимума, значение, история итераций.

	Исключения:
		ValueError: если не задана d2f.
		ZeroDivisionError: если f''(x) обращается в ноль на итерации.
	"""
	if d2f is None:
		raise ValueError("Для метода касательных требуется d2f (вторая производная)")

	history: List[Dict[str, Any]] = []
	x = float(x0)
	for _ in range(max_iter):
		g = float(df(x))
		h = float(d2f(x))
		history.append({"x": x, "df": g, "d2f": h, "f": float(f(x))})
		if abs(g) < tol:
			break
		if h == 0:
			raise ZeroDivisionError("d2f(x) = 0, метод Ньютона не применим")
		step = g / h
		x_new = x - step
		if abs(x_new - x) < tol:
			x = x_new
			break
		x = x_new

	x_min = x
	f_min = float(f(x_min))
	return OptimizationResult(
		x_min=x_min,
		f_min=f_min,
		iterations=len(history),
		history=history,
		method="Касательных (Ньютона)",
	)


def secant_on_gradient(
	f: Callable[[float], float],
	df: Optional[Callable[[float], float]],
	x0: float,
	x1: float,
	tol: float = 1e-6,
	max_iter: int = 200,
	h_fd: float = 1e-6,
) -> OptimizationResult:
	"""Метод секущих, применённый к уравнению f'(x) = 0 для минимума.

	Если аналитическая производная df недоступна, используется численное
	приближение производной по конечной разности:
		g(x) ≈ (f(x + h) - f(x - h)) / (2h)

	Итерации секущих: x_{k+1} = x_k - g(x_k) * (x_k - x_{k-1}) / (g(x_k) - g(x_{k-1})).

	Параметры:
		f: Callable[[float], float]
			Целевая функция.
		df: Optional[Callable[[float], float]]
			Первая производная, если доступна.
		x0: float
			Первое начальное приближение.
		x1: float
			Второе начальное приближение.
		tol: float
			Критерий остановки по |x_k - x_{k-1}| и |g(x_k)|.
		max_iter: int
			Максимум итераций.
		h_fd: float
			Шаг для численного дифференцирования, если df не задана.

	Возвращает:
		OptimizationResult: оценка минимума и история итераций.

	Исключения:
		— нет специальных, но возможна остановка по делению на малую разность градиентов.
	"""
	def grad(x: float) -> float:
		if df is not None:
			return float(df(x))
		return float((f(x + h_fd) - f(x - h_fd)) / (2.0 * h_fd))

	history: List[Dict[str, Any]] = []

	x_prev = float(x0)
	x_curr = float(x1)
	g_prev = grad(x_prev)
	g_curr = grad(x_curr)
	history.append({"x": x_prev, "g": g_prev, "f": float(f(x_prev))})
	history.append({"x": x_curr, "g": g_curr, "f": float(f(x_curr))})

	for _ in range(max_iter):
		if abs(g_curr - g_prev) < 1e-20:
			break
		x_next = x_curr - g_curr * (x_curr - x_prev) / (g_curr - g_prev)
		x_prev, g_prev = x_curr, g_curr
		x_curr = float(x_next)
		g_curr = grad(x_curr)
		history.append({"x": x_curr, "g": g_curr, "f": float(f(x_curr))})
		if abs(x_curr - x_prev) < tol or abs(g_curr) < tol:
			break

	x_min = x_curr
	f_min = float(f(x_min))
	return OptimizationResult(
		x_min=x_min,
		f_min=f_min,
		iterations=len(history),
		history=history,
		method="Секущих (по производной)",
	)


===== D:\_Учеба\_курсач_питон\app\optim\selection.py =====
# -*- coding: utf-8 -*-
"""Выбор и запуск подходящего метода одномерной оптимизации.

Функция auto_select_and_run инкапсулирует логику выбора алгоритма на
основании доступных данных: интервала, производных и стартовых точек.
"""
from __future__ import annotations

from typing import Callable, Optional, Tuple, Dict, Any

from .methods import (
	OptimizationResult,
	passive_search,
	dichotomy,
	golden_section,
	newton_tangent,
	secant_on_gradient,
)


def auto_select_and_run(
	f: Callable[[float], float],
	bounds: Optional[Tuple[float, float]] = None,
	tol: float = 1e-5,
	samples: Optional[int] = None,
	df: Optional[Callable[[float], float]] = None,
	d2f: Optional[Callable[[float], float]] = None,
	x0: Optional[float] = None,
	x1: Optional[float] = None,
	prefer: Optional[str] = None,
) -> OptimizationResult:
	"""Выбрать подходящий метод и выполнить оптимизацию.

	Правила выбора:
	- prefer задаёт принудительный метод ("passive", "dichotomy", "golden", "newton", "secant").
	- есть df и d2f и x0 → Ньютон;
	- есть два старта x0, x1 → секущие по производной (численной или аналитической);
	- задан только интервал → пассивный поиск (если указан samples) или золотое сечение.

	Параметры:
		f: Callable[[float], float]
			Целевая функция.
		bounds: Optional[Tuple[float, float]]
			Интервал поиска минимума.
		tol: float
			Точность для интервальных/производных методов.
		samples: Optional[int]
			Число проб для пассивного поиска.
		df: Optional[Callable[[float], float]]
			Первая производная (если доступна).
		d2f: Optional[Callable[[float], float]]
			Вторая производная (для Ньютона).
		x0: Optional[float]
			Начальное приближение.
		x1: Optional[float]
			Второе начальное приближение (для секущих).
		prefer: Optional[str]
			Явный выбор метода.

	Возвращает:
		OptimizationResult: результат выбранного метода с историей итераций.

	Исключения:
		ValueError: если данных недостаточно или противоречивы для выбранного метода.
	"""
	# Принудительный выбор
	if prefer is not None:
		key = prefer.lower()
		if key == "passive":
			if not bounds:
				raise ValueError("Для пассивного поиска нужен интервал bounds=(a,b)")
			if not samples:
				samples = 50
			return passive_search(f, bounds, samples)
		elif key == "dichotomy":
			if not bounds:
				raise ValueError("Для дихотомии нужен интервал bounds=(a,b)")
			return dichotomy(f, bounds, tol=tol)
		elif key == "golden":
			if not bounds:
				raise ValueError("Для золотого сечения нужен интервал bounds=(a,b)")
			return golden_section(f, bounds, tol=tol)
		elif key == "newton":
			if d2f is None or df is None or x0 is None:
				raise ValueError("Для Ньютона требуются df, d2f и начальное x0")
			return newton_tangent(f, df, d2f, x0=x0, tol=tol)
		elif key == "secant":
			if x0 is None or x1 is None:
				raise ValueError("Для метода секущих нужны два старта x0 и x1")
			return secant_on_gradient(f, df=df, x0=x0, x1=x1, tol=tol)
		else:
			raise ValueError("Неизвестное значение prefer")

	# Автовыбор
	if d2f is not None and df is not None and x0 is not None:
		return newton_tangent(f, df, d2f, x0=x0, tol=tol)

	if (x0 is not None) and (x1 is not None):
		return secant_on_gradient(f, df=df, x0=x0, x1=x1, tol=tol)

	if bounds is not None:
		if samples is not None:
			return passive_search(f, bounds, samples)
		# по умолчанию интервальный быстрый метод
		return golden_section(f, bounds, tol=tol)

	raise ValueError("Недостаточно данных для выбора метода. Укажите bounds или стартовые точки.")


===== D:\_Учеба\_курсач_питон\app\optim\__init__.py =====
# -*- coding: utf-8 -*-
"""Оптимизационные методы и вспомогательные функции выбора.

Смотрите:
- optim.methods — реализации алгоритмов и тип результата
- optim.selection — логика выбора и единая точка запуска
"""


===== D:\_Учеба\_курсач_питон\tests\test_advanced.py =====
# -*- coding: utf-8 -*-
"""Расширенные тесты: неунимодальные функции, шум и эффективность итераций."""
import math
import random
import numpy as np
import pytest

from app.optim.methods import (
	passive_search,
	golden_section,
	dichotomy,
)


def multimodal(x: float) -> float:
	"""Мультимодальная функция с несколькими минимумами на отрезке."""
	return math.sin(3*x) + 0.1*(x-1)**2


def noisy_quadratic(x: float) -> float:
	"""Слабо шумная квадратичная функция."""
	return (x-2.0)**2 + 3.0 + 1e-3*math.sin(50*x)


def test_multimodal_interval_methods_still_return_value():
	"""Интервальные методы должны возвращать конечные значения на мультимодальной функции."""
	res_g = golden_section(multimodal, (-2.0, 2.0), tol=1e-3)
	res_d = dichotomy(multimodal, (-2.0, 2.0), tol=1e-3)
	assert np.isfinite(res_g.f_min)
	assert np.isfinite(res_d.f_min)


def test_noisy_quadratic_interval_methods_robust():
	"""Интервальные методы сохраняют устойчивость при малом шуме."""
	res_g = golden_section(noisy_quadratic, (-5.0, 5.0), tol=1e-3)
	res_d = dichotomy(noisy_quadratic, (-5.0, 5.0), tol=1e-3)
	assert abs(res_g.x_min - 2.0) < 5e-2
	assert abs(res_d.x_min - 2.0) < 5e-2


def test_iteration_efficiency_golden_vs_passive():
	"""Золотое сечение обычно требует меньше итераций, чем пассивный поиск."""
	res_g = golden_section(lambda x: (x-1.2345)**2, (-10, 10), tol=1e-5)
	res_p = passive_search(lambda x: (x-1.2345)**2, (-10, 10), samples=401)
	assert res_g.iterations < res_p.iterations


===== D:\_Учеба\_курсач_питон\tests\test_methods.py =====
# -*- coding: utf-8 -*-
"""Тесты методов одномерной оптимизации (успех и обработка ошибок).

Включает параметризованные проверки на нескольких квадратичных функциях с
разными минимумами и масштабами.
"""
import math
import pytest

from app.optim.methods import (
	passive_search,
	dichotomy,
	golden_section,
	newton_tangent,
	secant_on_gradient,
)


def quad(x: float) -> float:
	"""Базовая квадратичная функция с минимумом в x=2, f=3."""
	return (x - 2.0) ** 2 + 3.0


def d_quad(x: float) -> float:
	"""Первая производная базовой квадратичной функции."""
	return 2.0 * (x - 2.0)


def d2_quad(x: float) -> float:
	"""Вторая производная базовой квадратичной функции."""
	return 2.0


def make_quad(c: float, shift: float, bias: float):
	"""Создать квадратичную функцию f(x)=c*(x-shift)^2 + bias и её производные."""
	def f(x: float) -> float:
		return c * (x - shift) ** 2 + bias
	def df(x: float) -> float:
		return 2.0 * c * (x - shift)
	def d2f(x: float) -> float:
		return 2.0 * c
	return f, df, d2f


def test_passive_search_basic():
	"""Пассивный поиск должен найти точный минимум на сетке 101 точка."""
	res = passive_search(quad, (-5.0, 5.0), samples=101)
	assert abs(res.x_min - 2.0) < 1e-12
	assert abs(res.f_min - 3.0) < 1e-12
	assert res.method == "Пассивный поиск"
	assert len(res.history) == 101


def test_passive_search_invalid_samples():
	"""Пассивный поиск: ошибка при samples < 2."""
	with pytest.raises(ValueError):
		passive_search(quad, (-1.0, 1.0), samples=1)


def test_dichotomy_converges():
	"""Дихотомия: сходимость к окрестности минимума при разумном tol."""
	res = dichotomy(quad, (-5.0, 5.0), tol=1e-4)
	assert abs(res.x_min - 2.0) < 5e-4
	assert abs(res.f_min - 3.0) < 1e-3
	assert res.method == "Дихотомия"
	assert len(res.history) >= 1


def test_dichotomy_invalid_tol():
	"""Дихотомия: tol<=0 недопустим."""
	with pytest.raises(ValueError):
		dichotomy(quad, (-1.0, 1.0), tol=0.0)


def test_golden_section_converges():
	"""Золотое сечение: сходимость к окрестности минимума."""
	res = golden_section(quad, (-5.0, 5.0), tol=1e-6)
	assert abs(res.x_min - 2.0) < 5e-4
	assert abs(res.f_min - 3.0) < 1e-3
	assert res.method == "Золотое сечение"


def test_newton_tangent_needs_d2f():
	"""Ньютон: без второй производной должен падать с ValueError."""
	with pytest.raises(ValueError):
		newton_tangent(quad, d_quad, None, x0=0.0)


def test_newton_tangent_converges_fast():
	"""Ньютон: быстро сходится на квадратичной функции."""
	res = newton_tangent(quad, d_quad, d2_quad, x0=0.0, tol=1e-12)
	assert abs(res.x_min - 2.0) < 1e-9
	assert abs(res.f_min - 3.0) < 1e-9
	assert res.iterations <= 5
	assert res.method == "Касательных (Ньютона)"


def test_secant_gradient_with_df():
	"""Секущие по аналитической производной: сходимость."""
	res = secant_on_gradient(quad, df=d_quad, x0=0.0, x1=1.0, tol=1e-10)
	assert abs(res.x_min - 2.0) < 1e-8
	assert abs(res.f_min - 3.0) < 1e-8
	assert res.method == "Секущих (по производной)"


def test_secant_gradient_numeric():
	"""Секущие по численному градиенту: сходимость."""
	res = secant_on_gradient(quad, df=None, x0=0.0, x1=1.0, tol=1e-8)
	assert abs(res.x_min - 2.0) < 1e-6
	assert abs(res.f_min - 3.0) < 1e-6


@pytest.mark.parametrize("c,shift,bias,bounds", [
	(1.0, 0.0, 0.0, (-10.0, 10.0)),
	(2.5, -3.0, 5.0, (-10.0, 10.0)),
	(0.5, 4.2, -7.0, (-10.0, 10.0)),
])
def test_all_methods_on_various_quadratics(c, shift, bias, bounds):
	"""Проверка всех методов на наборе квадратичных функций с разными параметрами."""
	f, df, d2f = make_quad(c, shift, bias)

	# Пассивный
	res = passive_search(f, bounds, samples=401)
	assert abs(res.x_min - shift) < 1e-2
	assert abs(res.f_min - bias) < 1e-1

	# Дихотомия
	res = dichotomy(f, bounds, tol=1e-4)
	assert abs(res.x_min - shift) < 5e-3
	assert abs(res.f_min - bias) < 1e-2

	# Золотое сечение
	res = golden_section(f, bounds, tol=1e-4)
	assert abs(res.x_min - shift) < 5e-3
	assert abs(res.f_min - bias) < 1e-2

	# Ньютон
	res = newton_tangent(f, df, d2f, x0=bounds[0], tol=1e-10)
	assert abs(res.x_min - shift) < 1e-8
	assert abs(res.f_min - bias) < 1e-8

	# Секущие
	res = secant_on_gradient(f, df=df, x0=bounds[0], x1=bounds[1], tol=1e-8)
	assert abs(res.x_min - shift) < 1e-6
	assert abs(res.f_min - bias) < 1e-6


===== D:\_Учеба\_курсач_питон\tests\test_selection.py =====
# -*- coding: utf-8 -*-
"""Тесты логики выбора метода (auto_select_and_run)."""
import pytest
from app.optim.selection import auto_select_and_run


def quad(x: float) -> float:
	"""Базовая квадратичная функция с минимумом в x=2."""
	return (x - 2.0) ** 2 + 3.0


def d_quad(x: float) -> float:
	"""Первая производная."""
	return 2.0 * (x - 2.0)


def d2_quad(x: float) -> float:
	"""Вторая производная (константа)."""
	return 2.0


def test_select_newton_when_derivatives_and_x0():
	res = auto_select_and_run(quad, bounds=None, df=d_quad, d2f=d2_quad, x0=0.0, tol=1e-10)
	assert abs(res.x_min - 2.0) < 1e-8
	assert res.method.startswith("Касательных")


def test_select_secant_when_two_starts():
	res = auto_select_and_run(quad, bounds=None, df=None, x0=0.0, x1=1.0, tol=1e-8)
	assert abs(res.x_min - 2.0) < 1e-6
	assert res.method.startswith("Секущих")


def test_select_passive_when_samples_given():
	res = auto_select_and_run(quad, bounds=(-5.0, 5.0), samples=21)
	assert res.method == "Пассивный поиск"


def test_select_golden_by_default_with_bounds():
	res = auto_select_and_run(quad, bounds=(-5.0, 5.0), tol=1e-5)
	assert res.method == "Золотое сечение"


def test_select_prefer_overrides():
	res = auto_select_and_run(quad, bounds=(-5.0, 5.0), tol=1e-5, prefer="dichotomy")
	assert res.method == "Дихотомия"


def test_select_raises_when_insufficient_data():
	with pytest.raises(ValueError):
		auto_select_and_run(quad)


===== D:\_Учеба\_курсач_питон\selftest.py =====
# -*- coding: utf-8 -*-
"""Self-test: проверка реализованных методов одномерной оптимизации.

Запускается так:
	python selftest.py
"""
from __future__ import annotations

from typing import Tuple

from app.optim.methods import (
	passive_search,
	dichotomy,
	golden_section,
	newton_tangent,
	secant_on_gradient,
)


def quad(x: float) -> float:
	return (x - 2.0) ** 2 + 3.0


def d_quad(x: float) -> float:
	return 2.0 * (x - 2.0)


def d2_quad(x: float) -> float:
	return 2.0


def run_all() -> None:
	bounds: Tuple[float, float] = (-5.0, 5.0)
	print("Function: (x-2)^2 + 3; true minimum at x*=2, f*=3\n")

	# Пассивный поиск
	res = passive_search(quad, bounds, samples=101)
	print(f"[Пассивный] x_min={res.x_min:.6f}, f_min={res.f_min:.6f}, iters={res.iterations}")

	# Дихотомия
	res = dichotomy(quad, bounds, tol=1e-6)
	print(f"[Дихотомия] x_min={res.x_min:.6f}, f_min={res.f_min:.6f}, iters={res.iterations}")

	# Золотое сечение
	res = golden_section(quad, bounds, tol=1e-6)
	print(f"[Золотое] x_min={res.x_min:.6f}, f_min={res.f_min:.6f}, iters={res.iterations}")

	# Ньютон (касательных)
	res = newton_tangent(quad, d_quad, d2_quad, x0=0.0, tol=1e-10)
	print(f"[Ньютон] x_min={res.x_min:.12f}, f_min={res.f_min:.12f}, iters={res.iterations}")

	# Секущие по производной (аналитическая df)
	res = secant_on_gradient(quad, df=d_quad, x0=0.0, x1=1.0, tol=1e-10)
	print(f"[Секущие df] x_min={res.x_min:.12f}, f_min={res.f_min:.12f}, iters={res.iterations}")

	# Секущие по численному градиенту
	res = secant_on_gradient(quad, df=None, x0=0.0, x1=1.0, tol=1e-10)
	print(f"[Секущие num] x_min={res.x_min:.12f}, f_min={res.f_min:.12f}, iters={res.iterations}")

	print("\nOK: методы отработали. Проверьте численные значения, ожидаем x~2, f~3.")


if __name__ == "__main__":
	run_all()


===== D:\_Учеба\_курсач_питон\main.py =====
# -*- coding: utf-8 -*-
"""Точка входа: запуск Streamlit-приложения через `python main.py`."""
import os
import sys
import subprocess


def main() -> int:
	"""Запустить Streamlit-приложение из файла app/app.py.

	Возвращает код завершения подпроцесса Streamlit.
	"""
	app_path = os.path.join(os.path.dirname(__file__), "app", "app.py")
	cmd = [sys.executable, "-m", "streamlit", "run", app_path]
	return subprocess.call(cmd)


if __name__ == "__main__":
	sys.exit(main())


